[[File:Vm-cluster.jpg|200px|thumb|right|GPU view]]
[[File:Network-namespace-in-linux.png|200px|thumb|right|Network namespace in linux, transposed to one name space = one processor on GPU]]

= Why we want virtual machine on GPU? =
We want virtual machine on GPU because it have at '''very low cost a large number of virtuals machines''' running. The memory bandwith is very large, large amount of CPU.

= Misconception =
One of misconception is put all CPU to all vm, do a big overload due to process balancing, fight for ressources, instable performance.

Prefer one CPU by virtual machine to have internal optimisation (SMP vs No SMP linux kernel)

Only some GPU part as network processing, only the software, will do constant context switch, bandwidth to pass the CPU <-> GPU datas, use power and do latency problem.

= MMU =
Your MMU, memory management unit should be able to run a kernel/user space and multi-user and processus.

= Conception =
[[File:Network-space.png|200px|thumb|right|vm by GPU processor]]

You need have light kernel by CPU of your GPU, and the kernel manage the local process. Each kernel will have unique network namespace, but don't need manage namespace because don't have multiple to manage. Each is responsive of their task, less fast than main CPU for single thread but lot of more processing power than the CPU for cluster (hadoop cluster, big data where the data don't fit in ram). No slow down on fast node when some other node is slow (high independency).

'''Unique big kernel will have scalability problem''', lock contention, some internal function in monothread, other need merge the result to unique node, ...

Then the internal GPU bus will be transformed into switch, don't matter if it's ring, lineare scale, tree conception (in this case you should group the vm by cluster to improve the interconnection).

The light kernel on GPU will be executed with another instruction than the machine CPU (x86), similarity with RPI (it start the GPU to start the CPU as device).

The main kernel is responsive of storage (direct hardware call? network FS?), complex syscall, and expose it to light kernel on GPU.

GPU receive directly the network stream, switch/route it to light kernel. Then the light kernel have all into the local memory and potentially use their L1 or other cache to grealy improve the [https://en.wikipedia.org/wiki/Instructions_per_second Instructions per second], targeting near billions of instructions per second or [https://en.wikipedia.org/wiki/FLOPS floating-point operations per second], it's decode the TCP stream, expose int high performance way to the software event loop (epoll force memory copy to read and write, need remake to have 0 copy process), the software manage it, and reply to it, the kernel recompose the TCP output stream. This with CPU <-> GPU exchange, without latency.

= Old to new =
[[File:Old-architecture.jpg|400px|thumb|left|Old architecture]] to [[File:New-architecture.jpg|400px|thumb|right|New architecture]]
<br style="clear:both">

= HSA =
<quote>[https://en.wikipedia.org/wiki/Heterogeneous_System_Architecture Heterogeneous System Architecture]  is a cross-vendor set of specifications that allow for the integration of central processing units and graphics processors on the same bus, with shared memory and tasks.[1] The HSA is being developed by the HSA Foundation, which includes (among many others) AMD and ARM. The platform's stated aim is to reduce communication latency between CPUs, GPUs and other compute devices, and make these various devices more compatible from a programmer's perspective,[2]:3[3] relieving the programmer of the task of planning the moving of data between devices' disjoint memories (as must currently be done with OpenCL or CUDA).[4]

Cuda and OpenCL as well as most other fairly advanced programming languages can use HSA to increase their execution performance.[5] Heterogeneous computing is widely used in system-on-chip devices such as tablets, smartphones, other mobile devices, and video game consoles.[6] HSA allows programs to use the graphics processor for floating point calculations without separate memory or scheduling.[7]

CF: wikipedia</quote>

The HSA is the first step of this, but for indirect interraction to network layer, need pass to main kernel and CPU, do general slow down, purge the cache, ...
